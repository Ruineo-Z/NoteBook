# AI News Daily å®ç°æ–¹æ¡ˆ

## æ¦‚è¿°

å®šæ—¶è·å– 4 ä¸ªå¹³å°çš„ AI èµ„è®¯ï¼Œæ±‡æ€»å¹¶ç¿»è¯‘æˆä¸­æ–‡ï¼Œæ¨é€ç»™ç”¨æˆ·ã€‚

## æ•°æ®æº

| å¹³å° | ç±»å‹ | è·å–æ–¹å¼ | ç­›é€‰é€»è¾‘ |
|------|------|----------|----------|
| Hacker News | æ–°é—»èšåˆ | RSS | å…³é”®è¯è¿‡æ»¤ï¼šAI, ML, LLM, OpenAI, GPT, Anthropic |
| ArXiv | å­¦æœ¯è®ºæ–‡ | API | åˆ†ç±»ï¼šcs.AI, cs.LGï¼ŒæŒ‰æ—¶é—´æ’åº |
| Twitter/X | ç¤¾äº¤åª’ä½“ | RSS | è®¢é˜… AI ç ”ç©¶è€…/å…¬å¸è´¦å· |
| ç¨€åœŸæ˜é‡‘ | ä¸­æ–‡ç¤¾åŒº | RSS | AI æ ‡ç­¾ä¸‹çš„æ–‡ç«  |

## æŠ€æœ¯æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      å®šæ—¶è§¦å‘å™¨                              â”‚
â”‚                  ï¼ˆæ¯å¤©æ—©ä¸Š 9:00ï¼‰                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     News Fetcher                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ HN RSS   â”‚ â”‚ ArXiv APIâ”‚ â”‚ Twitter  â”‚ â”‚ ç¨€åœŸæ˜é‡‘ â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  RSS     â”‚ â”‚  RSS     â”‚       â”‚
â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Data Processor                            â”‚
â”‚  â€¢ è§£æ XML/JSON                                             â”‚
â”‚  â€¢ å»é‡ï¼ˆæ ‡é¢˜ç›¸ä¼¼åº¦åˆ¤æ–­ï¼‰                                     â”‚
â”‚  â€¢ å…³é”®è¯è¿‡æ»¤                                                â”‚
â”‚  â€¢ æ—¶é—´è¿‡æ»¤ï¼ˆè¿‘ 24 å°æ—¶ï¼‰                                    â”‚
â”‚  â€¢ é‡è¦æ€§æ’åº                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LLM Processor                             â”‚
â”‚  â€¢ ç¿»è¯‘æ ‡é¢˜/æ‘˜è¦ä¸ºä¸­æ–‡                                       â”‚
â”‚  â€¢ ç”Ÿæˆç®€æ´æ‘˜è¦ï¼ˆ2-3 å¥è¯ï¼‰                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Output                                  â”‚
â”‚  æ ¼å¼ï¼šMarkdown / é£ä¹¦æ¶ˆæ¯ / å¾®ä¿¡                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## å„å¹³å°å®ç°ç»†èŠ‚

### 1. Hacker News

```python
# é…ç½®
RSS_URL = "https://news.ycombinator.com/rss"

# å…³é”®è¯åˆ—è¡¨
HN_KEYWORDS = [
    "AI", "ML", "machine learning", "LLM", "large language model",
    "OpenAI", "GPT", "Anthropic", "Claude", "DeepMind",
    "neural network", "transformer", "generative AI"
]

# è·å–é€»è¾‘
def fetch_hn_news():
    response = requests.get(RSS_URL)
    items = parse_xml(response.text)
    filtered = [item for item in items
                if any(kw.lower() in item.title.lower() for kw in HN_KEYWORDS)]
    return filtered[:10]
```

### 2. ArXiv

```python
# é…ç½®
API_URL = "http://export.arxiv.org/api/query"

# æŸ¥è¯¢å‚æ•°
QUERY_PARAMS = {
    "search_query": "cat:cs.AI OR cat:cs.LG OR cat:cs.CL OR cat:cs.NE",
    "start": 0,
    "max_results": 20,
    "sortBy": "submittedDate",
    "sortOrder": "descending"
}

# è·å–é€»è¾‘
def fetch_arxiv_papers():
    response = requests.get(API_URL, params=QUERY_PARAMS)
    feed = parse_atom(response.text)
    papers = []
    for entry in feed.entries:
        papers.append({
            "title": entry.title,
            "summary": entry.summary,
            "authors": [a.name for a in entry.authors],
            "pdf_url": entry.id.replace("abs", "pdf") + ".pdf",
            "published": entry.published
        })
    return papers
```

### 3. Twitter/X

```python
# æ–¹æ¡ˆï¼šä½¿ç”¨ RSSHub æˆ– Nitter
RSSHUB_URL = "https://rsshub.app/twitter/following"

# å…³æ³¨çš„ AI è´¦å·åˆ—è¡¨
TWITTER_ACCOUNTS = [
    "@OpenAI",
    "@anthropic",
    "@ylecun",
    "@JeffDean",
    "@AndrewYNg",
    "@ylecun",
    "@hardmaru"
]

# è·å–é€»è¾‘
def fetch_twitter_news():
    # æ–¹å¼1ï¼šRSSHub è®¢é˜…åˆ—è¡¨
    url = f"{RSSHUB_URL}?limit=20"
    response = requests.get(url)
    items = parse_rss(response.text)
    return items
```

### 4. ç¨€åœŸæ˜é‡‘

```python
# é…ç½®
RSS_URL = "https://juejin.cn/rss/posts/6814622904592242719"  # AI æ ‡ç­¾

# è·å–é€»è¾‘
def fetch_juejin_news():
    response = requests.get(RSS_URL)
    items = parse_xml(response.text)
    return items[:10]
```

## æ•°æ®å¤„ç†

### å»é‡ç­–ç•¥

```python
from difflib import SequenceMatcher

def is_duplicate(title1, title2, threshold=0.8):
    """åˆ¤æ–­ä¸¤ä¸ªæ ‡é¢˜æ˜¯å¦é‡å¤"""
    return SequenceMatcher(None, title1.lower(), title2.lower()).ratio() > threshold

def deduplicate(news_list):
    """å»é‡"""
    unique_news = []
    for news in news_list:
        is_dup = False
        for existing in unique_news:
            if is_duplicate(news["title"], existing["title"]):
                # ä¿ç•™å†…å®¹æ›´ä¸°å¯Œçš„é‚£æ¡
                if len(news.get("summary", "")) > len(existing.get("summary", "")):
                    unique_news.remove(existing)
                    unique_news.append(news)
                is_dup = True
                break
        if not is_dup:
            unique_news.append(news)
    return unique_news
```

### é‡è¦æ€§æ’åº

```python
def calculate_importance(news):
    """è®¡ç®—æ–°é—»é‡è¦æ€§åˆ†æ•°"""
    score = 0

    # æ ‡é¢˜åŒ…å«é‡è¦å…³é”®è¯
    important_keywords = ["release", "announce", "new", "GPT-4", "Claude 3", "GPT-5"]
    if any(kw in news["title"] for kw in important_keywords):
        score += 5

    # æ¥æºæƒé‡
    source_weights = {
        "OpenAI": 10,
        "Anthropic": 10,
        "Google DeepMind": 8,
        "Meta AI": 6,
        "ArXiv": 4,
        "Hacker News": 3
    }
    score += source_weights.get(news.get("source", ""), 1)

    return score
```

## è¾“å‡ºæ ¼å¼

```markdown
# ğŸ“° ä»Šæ—¥ AI è¦é—»

> æ±‡æ€»æ—¶é—´ï¼š2024-01-30 09:00

## 1. [OpenAI å‘å¸ƒ GPT-4.5](https://...)
   **æ‘˜è¦**ï¼šOpenAI æ­£å¼æ¨å‡º GPT-4.5ï¼Œåœ¨æ¨ç†èƒ½åŠ›å’Œæˆæœ¬æ•ˆç‡ä¸Šæœ‰æ˜¾è‘—æå‡...
   **æ¥æº**ï¼šOpenAI Blog
   **æ ‡ç­¾**ï¼šäº§å“å‘å¸ƒ

## 2. [Meta å‘å¸ƒ Llama 3](https://...)
   **æ‘˜è¦**ï¼šMeta å®£å¸ƒå¼€æº Llama 3 æ¨¡å‹ï¼Œæ€§èƒ½æ¥è¿‘ GPT-4...
   **æ¥æº**ï¼šTwitter @MetaAI
   **æ ‡ç­¾**ï¼šå¼€æºæ¨¡å‹

---

ğŸ’¡ å…± 5 æ¡æ–°é—» | æ¥æºï¼šHN(2) + ArXiv(1) + Twitter(1) + ç¨€åœŸæ˜é‡‘(1)
```

## å®šæ—¶ä»»åŠ¡é…ç½®

### Cron è¡¨è¾¾å¼

```bash
# æ¯å¤©æ—©ä¸Š 9:00 è¿è¡Œ
0 9 * * * /usr/bin/python3 /path/to/ai_news_daily.py
```

### GitHub Actionsï¼ˆå¯é€‰ï¼‰

```yaml
name: Daily AI News
on:
  schedule:
    - cron: '0 9 * * *'
  workflow_dispatch:

jobs:
  fetch-news:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Run AI News Fetcher
        run: python3 ai_news_daily.py
```

## å¾…åŠ

- [ ] å®ç° HN RSS è§£ææ¨¡å—
- [ ] å®ç° ArXiv API è§£ææ¨¡å—
- [ ] å®ç° Twitter RSS è§£ææ¨¡å—
- [ ] å®ç°ç¨€åœŸæ˜é‡‘ RSS è§£ææ¨¡å—
- [ ] å®ç°æ•°æ®å»é‡å’Œæ’åºé€»è¾‘
- [ ] é›†æˆ LLM ç¿»è¯‘å’Œæ‘˜è¦ç”Ÿæˆ
- [ ] é…ç½®å®šæ—¶ä»»åŠ¡
- [ ] å¯¹æ¥é£ä¹¦/å¾®ä¿¡æ¨é€

## å‚è€ƒèµ„æ–™

- [Hacker News RSS](https://news.ycombinator.com/rss)
- [ArXiv API](http://export.arxiv.org/api_help)
- [RSSHub](https://docs.rsshub.app/)
- [ç¨€åœŸæ˜é‡‘ RSS](https://juejin.cn/rss)
